import os
import torch
import onnx
import numpy as np
import warnings

# è¿‡æ»¤OpenVINO runtimeå¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning, message=".*openvino.runtime.*")
# æ–°ç‰ˆOpenVINO 2024.x+ æ­£ç¡®å¯¼å…¥æ–¹å¼
import openvino as ov

# å¯¼å…¥ä½ çš„æ¨¡å‹ç±»ï¼ˆç¡®ä¿model.pyåœ¨å½“å‰ç›®å½•ï¼‰
from model import YOLO11ROIClassifier

# ===================== é…ç½®å‚æ•° =====================
DEVICE = torch.device("cpu")
MODEL_SIZE = "s"
NUM_ROI = 12
NUM_CLASSES = 3
ROI_SIZE = 64

# æ–‡ä»¶è·¯å¾„ï¼ˆåŸå§‹å­—ç¬¦ä¸²é¿å…è½¬ä¹‰ï¼‰
PT_MODEL_PATH = r"H:\pycharm\yolov11\yolov11_proj1\yolo11_Custom_12roi\model_pt\yolo11s_roi_16334.pt"
ONNX_MODEL_PATH = r"H:\pycharm\yolov11\yolov11_proj1\yolo11_Custom_12roi\model_onnx\yolo11s_roi_16334.onnx"
OPENVINO_IR_PATH = r"H:\pycharm\yolov11\yolov11_proj1\yolo11_Custom_12roi\model_openvino\yolo11s_roi_16334"

# è¾“å…¥é…ç½®
BATCH_SIZE = 1
INPUT_SHAPE = (BATCH_SIZE, NUM_ROI, 3, ROI_SIZE, ROI_SIZE)
INPUT_NAME = "roi_imgs"


# ===================== 1. åŠ è½½PyTorchæ¨¡å‹ =====================
def load_pytorch_model(pt_path, model_size, num_roi, num_classes, device):
    model = YOLO11ROIClassifier(
        model_size=model_size,
        num_roi=num_roi,
        num_classes=num_classes,
        roi_size=ROI_SIZE
    ).to(device)

    checkpoint = torch.load(pt_path, map_location=device)
    if "model_state_dict" in checkpoint:
        model_weights = checkpoint["model_state_dict"]
    elif "state_dict" in checkpoint:
        model_weights = checkpoint["state_dict"]
    else:
        model_weights = checkpoint

    model.load_state_dict(model_weights, strict=False)
    model.eval()
    print(f"âœ… PyTorchæ¨¡å‹åŠ è½½å®Œæˆï¼š{pt_path}")
    return model


# ===================== 2. PTè½¬ONNX =====================
def convert_pt_to_onnx(model, onnx_path, input_shape, device):
    # æå‰åˆ›å»ºç›®å½•
    onnx_dir = os.path.dirname(onnx_path)
    os.makedirs(onnx_dir, exist_ok=True)

    dummy_input = torch.randn(input_shape, device=device)

    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        opset_version=12,
        do_constant_folding=True,
        input_names=[INPUT_NAME],
        output_names=["pred_logits"],
        dynamic_axes=None
    )

    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print(f"âœ… ONNXæ¨¡å‹å¯¼å‡ºå¹¶éªŒè¯å®Œæˆï¼š{onnx_path}")


# ===================== 3. ONNXè½¬OpenVINO IR =====================
def convert_onnx_to_openvino(onnx_path, ir_path):
    # æå‰åˆ›å»ºç›®å½•
    ir_dir = os.path.dirname(ir_path)
    os.makedirs(ir_dir, exist_ok=True)

    # æ ¸å¿ƒä¿®å¤ï¼šç§»é™¤data_typeå‚æ•°ï¼ˆ2024.x+ç‰ˆæœ¬å·²ç§»é™¤ï¼‰
    ov_model = ov.convert_model(
        input_model=onnx_path,
        input={INPUT_NAME: INPUT_SHAPE}
    )

    # ä¿å­˜IRæ¨¡å‹ï¼ˆæ–°ç‰ˆAPIï¼‰
    ov.save_model(ov_model, ir_path + ".xml")
    print(f"âœ… OpenVINO IRæ¨¡å‹è½¬æ¢å®Œæˆï¼š{ir_path}.xml / {ir_path}.bin")

    # éªŒè¯åŠ è½½
    core = ov.Core()
    compiled_model = core.compile_model(ov_model, "CPU")
    print(f"âœ… OpenVINOæ¨¡å‹ç¼–è¯‘éªŒè¯å®Œæˆï¼ˆè®¾å¤‡ï¼šCPUï¼‰")
    return compiled_model


# ===================== 4. æ¨ç†éªŒè¯ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šset_input_tensorå‚æ•°ï¼‰ =====================
def inference_verify(compiled_model, input_shape):
    """éªŒè¯è½¬æ¢åçš„OpenVINOæ¨¡å‹æ¨ç†æ­£ç¡®æ€§ï¼ˆé€‚é…2024.x+ APIï¼‰"""
    # æ„é€ æµ‹è¯•è¾“å…¥ï¼ˆnumpyæ•°ç»„ï¼‰
    test_input = np.random.randn(*input_shape).astype(np.float32)

    # æ ¸å¿ƒä¿®å¤1ï¼šå°†numpyæ•°ç»„è½¬æ¢ä¸ºOpenVINO Tensorå¯¹è±¡
    input_tensor = ov.Tensor(test_input)

    # æ ¸å¿ƒä¿®å¤2ï¼šè°ƒç”¨set_input_tensorï¼ˆå‚æ•°ä¸ºç´¢å¼• + Tensorï¼Œæˆ–ç›´æ¥ä¼ Tensorï¼‰
    infer_request = compiled_model.create_infer_request()
    infer_request.set_input_tensor(0, input_tensor)  # 0=è¾“å…¥ç´¢å¼•ï¼Œinput_tensor=OpenVINO Tensor

    # æ‰§è¡Œæ¨ç†
    infer_request.infer()

    # è·å–è¾“å‡º
    output = infer_request.get_output_tensor(0).data  # 0=è¾“å‡ºç´¢å¼•
    print(f"âœ… æ¨ç†éªŒè¯å®Œæˆï¼š")
    print(f"   è¾“å…¥å½¢çŠ¶ï¼š{test_input.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶ï¼š{output.shape} (é¢„æœŸï¼š{[BATCH_SIZE, NUM_ROI, NUM_CLASSES]})")


# ===================== ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    # åŠ è½½PTæ¨¡å‹
    model = load_pytorch_model(
        pt_path=PT_MODEL_PATH,
        model_size=MODEL_SIZE,
        num_roi=NUM_ROI,
        num_classes=NUM_CLASSES,
        device=DEVICE
    )

    # PTè½¬ONNX
    convert_pt_to_onnx(
        model=model,
        onnx_path=ONNX_MODEL_PATH,
        input_shape=INPUT_SHAPE,
        device=DEVICE
    )

    # ONNXè½¬OpenVINO IR
    compiled_ov_model = convert_onnx_to_openvino(
        onnx_path=ONNX_MODEL_PATH,
        ir_path=OPENVINO_IR_PATH
    )

    # æ¨ç†éªŒè¯ï¼ˆä¿®å¤åï¼‰
    inference_verify(compiled_ov_model, INPUT_SHAPE)

    print("\nğŸ‰ æ‰€æœ‰è½¬æ¢æ­¥éª¤å®Œæˆï¼")
    print(f"   ONNXæ¨¡å‹ï¼š{ONNX_MODEL_PATH}")
    print(f"   OpenVINO IRæ¨¡å‹ï¼š{OPENVINO_IR_PATH}.xml / {OPENVINO_IR_PATH}.bin")
